{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7122a7f8-20c5-4722-ae0b-e6628f9526e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sys\n",
    "import tensorflow_probability as tfp\n",
    "    \n",
    "def label_clusters_by_elbow(val, plot = False, ax = None, std = None):\n",
    "    '''label clusters by model mean\n",
    "    val: sorted model mean\n",
    "    '''\n",
    "    \n",
    "    r_square = {}\n",
    "    reg_params = {}\n",
    "    for breakpoint in range(1, len(val)-1):\n",
    "        \n",
    "        left = val[:breakpoint+1]\n",
    "        right = val[breakpoint:]\n",
    "        \n",
    "        left_x = np.arange(start = 0, stop = len(left), step = 1)\n",
    "        left_reg = stats.linregress(left_x, left)\n",
    "        right_x = np.arange(start = breakpoint, stop = breakpoint+len(right), step = 1)\n",
    "        right_reg = stats.linregress(right_x, right)\n",
    "        r_square[breakpoint] = left_reg.rvalue**2+right_reg.rvalue**2\n",
    "        reg_params[breakpoint] = (left_x, left_reg, right_x, right_reg)\n",
    "        \n",
    "    elbow_point = max(r_square, key=r_square.get)\n",
    "    left_x, left_reg, right_x, right_reg = reg_params[elbow_point]\n",
    "    \n",
    "    selected = val.iloc[elbow_point+1:]\n",
    "    #print(val.loc[val-left_y>0].sort_values())\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        if ax is None:\n",
    "            f, ax = plt.subplots()\n",
    "        left_x, left_reg, right_x, right_reg = reg_params[elbow_point]\n",
    "        left_y = left_reg.intercept + left_reg.slope*left_x\n",
    "        left_y_upper_bound = (left_reg.intercept+ left_reg.intercept_stderr) + (left_reg.slope+left_reg.stderr)*left_x\n",
    "        left_y_lower_bound = (left_reg.intercept- left_reg.intercept_stderr) + (left_reg.slope-left_reg.stderr)*left_x\n",
    "        ax.plot(left_x, left_y, 'tomato', label='left regression', lw = 3)\n",
    "        ax.fill_between(left_x, left_y_upper_bound, left_y_lower_bound, color = 'pink', alpha = 0.5)\n",
    "        ax.plot(right_x, right_reg.intercept + right_reg.slope*right_x, 'orchid', label='right regression', lw = 3)\n",
    "        val.plot(marker = '+', color = 'grey', ax = ax, label = 'p_bar')\n",
    "        if std is not None:\n",
    "            ax.fill_between(np.arange(len(val)), val+3*std, val-3*std, color = 'skyblue', alpha = 0.5)\n",
    "        ax.vlines(x = elbow_point, ymax = val.max(), ymin = 0, color = 'black', linestyle='dashed')\n",
    "        ax.set_xticks(range(len(val)))\n",
    "        ax.set_xticklabels(val.index, rotation = 90)\n",
    "        ax.set_ylabel('$bar{p_{jk}}')\n",
    "        ax.set_title(val.name)\n",
    "    return selected\n",
    "\n",
    "def annotate_clusters(model_mean, plot = False, model_std=None):\n",
    "    cluster_assignment_df = []\n",
    "    f, axes = plt.subplots(math.ceil(model_mean.shape[0]/2),2, figsize = (12,16))\n",
    "    axes = axes.flatten()\n",
    "    for rbp, ax in zip(model_mean.index, axes):\n",
    "\n",
    "        val = model_mean.loc[rbp].sort_values()\n",
    "        if model_std is not None:\n",
    "            std = model_std.loc[rbp, val.index]\n",
    "        else:\n",
    "            std = None\n",
    "        selected = label_clusters_by_elbow(val, plot = plot, ax = ax, std=std)\n",
    "        selected = pd.Series([True]*len(selected), index = selected.index)\n",
    "        selected.name = rbp\n",
    "        cluster_assignment_df.append(selected)\n",
    "    ax.legend()\n",
    "    cluster_assignment_df = pd.concat(cluster_assignment_df, axis = 1).T\n",
    "    cluster_assignment_df.fillna(False, inplace = True)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    \n",
    "    missing_annotation = list(set(model_mean.columns)-set(cluster_assignment_df.columns))\n",
    "    cluster_assignment_df[missing_annotation]=False\n",
    "    return cluster_assignment_df.T\n",
    "\n",
    "def summaries_annotation_and_mean(anno, model_mean, model_var, read_dist):\n",
    "    ''' create cluster summary statistics '''\n",
    "    stats = []\n",
    "    for index, row in anno.iterrows():\n",
    "        #print(index)\n",
    "        rbps = row.loc[row].index\n",
    "        rbps_prefix_free = [r.split('.')[1] for r in rbps]\n",
    "        mean = model_mean.loc[rbps, index].sum()\n",
    "        var = model_var.loc[rbps, index].sum()\n",
    "        bg_mean = read_dist.loc[rbps].sum()\n",
    "        fc = mean/bg_mean\n",
    "        \n",
    "        comp_mean = model_mean[index].copy()\n",
    "        comp_mean.index = [i.split('.')[1] for i in comp_mean.index]\n",
    "        ri_contri = (comp_mean*np.log(comp_mean/read_dist))[rbps].sum()\n",
    "        \n",
    "        stat = [','.join(rbps_prefix_free), mean, var, bg_mean, fc, ri_contri]\n",
    "        stats.append(stat)\n",
    "    return pd.DataFrame(stats, index = anno.index, columns = ['RBP', 'RBP_mean', 'RBP_var', 'background_mean', 'fold_change', 'RI_contribution'])\n",
    "\n",
    "def count_by_rbp(data_merged, anno, to_count = 'feature_type_top'):\n",
    "    feature_counts = []\n",
    "    for rbp in anno.columns:\n",
    "        cnt = data_merged.loc[data_merged[rbp], to_count].value_counts()\n",
    "        cnt.name = rbp.split('.')[1]\n",
    "        feature_counts.append(cnt)\n",
    "    feature_counts = pd.concat(feature_counts, axis = 1).fillna(0).T\n",
    "    return feature_counts\n",
    "\n",
    "def get_counts_by_rbp(data, anno, data_col_to_merge = 'cluster', to_count = 'feature_type_top'):\n",
    "    ''' count feature types by each RBP '''\n",
    "    data_merged = data.merge(anno, left_on = data_col_to_merge, right_index = True, how = 'left')\n",
    "    data_merged[anno.columns] = data_merged[anno.columns].fillna(False)\n",
    "    \n",
    "    feature_counts = count_by_rbp(data_merged, anno, to_count = to_count)\n",
    "    return data_merged, feature_counts\n",
    "\n",
    "def compute_jaccard_index(identity_tbl):\n",
    "    ''' use jaccard index to show how much binding site overlapped between RBPs'''\n",
    "    d_condense = pdist(identity_tbl.T, 'jaccard')\n",
    "    d = pd.DataFrame(1-squareform(d_condense), index = identity_tbl.columns, columns = identity_tbl.columns)\n",
    "    return d\n",
    "\n",
    "\n",
    "class Dirichlet_Mixture_Model:\n",
    "    def __init__(self, alphas, weights, n):\n",
    "        \n",
    "        self.alphas = alphas # (alpha, beta) of each component, K*2 matrix\n",
    "        self.weights = weights # vector of length K\n",
    "        self.n = n\n",
    "        \n",
    "        self.distributions = []\n",
    "        \n",
    "        for a in self.alphas:\n",
    "            self.distributions.append(tfp.distributions.DirichletMultinomial(n, a))\n",
    "    def pmf(self,k):\n",
    "        ''' return pmf of  mixture model'''\n",
    "        p = 0\n",
    "        for w, dist in zip(self.weights, self.distributions):\n",
    "            p += w*dist.prob(k)\n",
    "        return p\n",
    "    \n",
    "    def cdf(self,k):\n",
    "        raise NotImplementedError\n",
    "    def logpmf(self,k):\n",
    "        return np.log(self.pmf(k))\n",
    "    def pvalue(self,k):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def DMM_bayes_factor(model_alphas, weights, rbps, components, raw_data, plot = True, nread = 30):\n",
    "    '''dirichlet multinomial mixture log likelihood (BF) by aggregation property\n",
    "    aggregate all the other proteins as noise\n",
    "    calculate P(X|alt)/P(X|null)\n",
    "    '''\n",
    "    \n",
    "    if weights.index[0] in model_alphas.columns:\n",
    "        model_alphas.columns = [f'alpha.{c}' for c in model_alphas.columns]\n",
    "    \n",
    "    # marginalize into [rbp], [other]\n",
    "    marginalized_alphas = pd.concat([model_alphas.loc[rbps].T, \n",
    "                                 model_alphas.loc[~model_alphas.index.isin(rbps)].sum(axis = 0)],\n",
    "                               axis = 1)\n",
    "    \n",
    "    marginalized_counts = pd.concat([raw_data[rbps], \n",
    "                                 raw_data.loc[:, ~raw_data.columns.isin(rbps)].sum(axis = 1)],\n",
    "                               axis = 1)\n",
    "    \n",
    "    \n",
    "    comp_names = weights.loc[components].index # [V22]\n",
    "    other_names = weights.loc[~weights.index.isin(components)].index # [all other comp]\n",
    "    \n",
    "    if 'alpha' in model_alphas.columns[0]:\n",
    "        comp_names_alpha= [f'alpha.{c}' for c in comp_names]\n",
    "        other_names_alpha = [f'alpha.{c}' for c in other_names]\n",
    "    \n",
    "    # reweight\n",
    "    alt_w = weights.loc[comp_names, 'pi']\n",
    "    alt_w = alt_w/alt_w.sum()\n",
    "    alt = Dirichlet_Mixture_Model(marginalized_alphas.loc[comp_names_alpha].values,\n",
    "                             alt_w,\n",
    "                            n = marginalized_counts.sum(axis = 1).astype(float))\n",
    "    \n",
    "    \n",
    "    null_w = weights.loc[other_names, 'pi']\n",
    "    null_w = null_w/null_w.sum()\n",
    "    null = Dirichlet_Mixture_Model(marginalized_alphas.loc[other_names_alpha].values,\n",
    "                             null_w,\n",
    "                             n = marginalized_counts.sum(axis = 1).astype(float))\n",
    "    \n",
    "    logL_comp = np.log(alt.pmf(marginalized_counts))\n",
    "    logL_null = np.log(null.pmf(marginalized_counts))\n",
    "    \n",
    "    \n",
    "    logLR = logL_comp-logL_null\n",
    "    \n",
    "    bf_df = pd.DataFrame([logL_null, logL_comp, logLR], \n",
    "                         index = ['logL_singlecomp', 'logL_comp', 'logLR'], \n",
    "                         columns = raw_data.index).T\n",
    "    \n",
    "    \n",
    "    return bf_df\n",
    "\n",
    "def filter_by_bf(bfs, individual_bfs, data, anno, comp_mapping = None, logbf_thres = 2):\n",
    "    ''' filter hypothesis assignment by hypothesis-wise BF and individual BF \n",
    "    hypothesis BF: ex, How likely is this window being SF3B4+PRPF8 instead of all other hypothesis (including SF3B4 sole binding and PRPF8 sole binding)\n",
    "    individual BF: ex, How likely is this window bound by SF3B4 (regardless of other partners' present) vs not being bound at all?\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    \n",
    "    # filtering for combinatorial binding\n",
    "    for index, row in data.iterrows():\n",
    "        clus = row['cluster']\n",
    "        if comp_mapping:\n",
    "            try:\n",
    "                evi = bfs.loc[index, comp_mapping[clus]]\n",
    "                rbps = anno.loc[comp_mapping[clus]][anno.loc[comp_mapping[clus]]].index\n",
    "            except:\n",
    "                evi = 0\n",
    "        else:\n",
    "            try:\n",
    "                evi = bfs.loc[index, clus]\n",
    "                rbps = anno.loc[clus][anno.loc[clus]].index\n",
    "            except:\n",
    "                evi = 0 # some of the clusters are not there anymore\n",
    "        \n",
    "        if individual_bfs.loc[index, rbps].ge(logbf_thres).all():\n",
    "            # if all RBPs are bound for specific component\n",
    "            data.loc[index, 'logLR']=evi\n",
    "        else:\n",
    "            data.loc[index, 'logLR']=0 # individual RBPs don't have evidence\n",
    "        \n",
    "        \n",
    "        \n",
    "    data.loc[data['logLR']>logbf_thres, 'BF_assignment'] = data.loc[data['logLR']>logbf_thres, 'cluster']\n",
    "    \n",
    "    data_merged, feature_counts = get_counts_by_rbp(data, anno, data_col_to_merge = 'BF_assignment')\n",
    "    \n",
    "    # rescue individual binding sites that were that detected by any hypothesis\n",
    "    not_assigned = data_merged.loc[data_merged['BF_assignment'].isnull()]\n",
    "    return data_merged, feature_counts\n",
    "\n",
    "def mask_megaoutput(megaoutputs, mask):\n",
    "    '''\n",
    "    take megaoutput, modify logLR:{RBP} and binary {RBP} labels based on log_cdf and mask\n",
    "    mask: True or false based on whether y_dev > zscore_cutoff * stdev\n",
    "    '''\n",
    "    # binary labels are gone\n",
    "    megaoutputs_masked = megaoutputs.copy()\n",
    "    common_index = list(set(megaoutputs_masked.index).intersection(set(mask.index)))\n",
    "    megaoutputs_masked.loc[common_index, mask.columns] = megaoutputs.loc[common_index, mask.columns] & mask # being logLR > 2 and >2*stdev\n",
    "    \n",
    "    return megaoutputs_masked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2016bf-ff59-40be-b95e-a5a8135dbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "cd /home/hsher/scratch/ABC_2rep/\n",
    "conda activate tensorflow_prob\n",
    "python /tscc/nfs/home/hsher/Mudskipper/scripts/analyze_DMM.py K562_rep6.1\n",
    "'''\n",
    "\n",
    "out_stem = 'K562_rep6'\n",
    "basedir = Path('/tscc/nfs/home/hsher/ps-yeolab5/ABC_2rep_rerun/')\n",
    "dmm_param_dir = Path('/tscc/nfs/home/hsher//ps-yeolab5/DMM_K_params/')\n",
    "\n",
    "def analyze_dmm_prefix(dmm_param_dir, dmm_prefix):\n",
    "\n",
    "    # constants\n",
    "    logLR_threshold = 2\n",
    "    ent_thres = 0.1\n",
    "    fc_raw_thres = 1\n",
    "    \n",
    "    # fitted parameters and outputs\n",
    "    data = pd.read_csv(dmm_param_dir/f'DMM/{dmm_prefix}.mixture_weight.tsv', sep = '\\t', index_col = 0) # basedir/f'DMM/{out_stem}.mixture_weight.tsv'\n",
    "    data.set_index('Row.names', inplace = True)\n",
    "    \n",
    "    mixture_weight_only = data.loc[:, data.columns.str.startswith('V')]\n",
    "    mixture_weight_only.columns\n",
    "    data['cluster']=mixture_weight_only.idxmax(axis = 1)\n",
    "    \n",
    "    weights = pd.read_csv(dmm_param_dir/f'DMM/{dmm_prefix}.weights.tsv', sep = '\\t', index_col = 0)\n",
    "    weights.index = [f'V{i}' for i in weights.index]\n",
    "    \n",
    "    model_alphas = pd.read_csv(dmm_param_dir/f'DMM/{dmm_prefix}.alpha.tsv', sep = '\\t',\n",
    "                        index_col = 0) # RBP by components, B * K\n",
    "    model_alphas.columns = [i.replace('X', 'V') for i in model_alphas.columns]\n",
    "    model_mean = model_alphas.div(model_alphas.sum(axis = 0), axis = 1)\n",
    "    model_var = model_alphas.apply(lambda column: dirichlet(column).var(), axis = 0)\n",
    "    model_std = np.sqrt(model_var)\n",
    "    \n",
    "    # masks\n",
    "    mask = pd.read_csv(basedir / 'mask' / f'{out_stem}.genome_mask.csv', index_col = 0)\n",
    "    \n",
    "    # raw_counts to calculate bayes factor\n",
    "    raw_counts = pd.read_csv(basedir /f'counts/genome/megatables/{out_stem}.tsv.gz', sep = '\\t')\n",
    "    raw_counts.index = raw_counts.index+1\n",
    "    raw_counts = raw_counts.loc[data.index] # filter for those that has been modelled (passing the total_read_threshold)\n",
    "    \n",
    "    # find total mapped reads as the null\n",
    "    nread_per_window = raw_counts.sum(axis = 1)\n",
    "    mapped_reads = raw_counts.sum(axis = 0)\n",
    "    mapped_reads_fraction = mapped_reads.div(mapped_reads.sum())\n",
    "    print('========Fraction mapped reads: \\n ========',mapped_reads_fraction.sort_values())\n",
    "    \n",
    "    # visualization\n",
    "    sns.clustermap(model_mean.T,\n",
    "            cbar_kws = {'label': '\\bar{p}'},\n",
    "            metric = 'correlation', cmap = 'Greys', figsize = (4,4))\n",
    "    plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.model_mean.pdf')\n",
    "    \n",
    "    \n",
    "    # calculate FC over null for each component\n",
    "    component_fc = (model_mean).div(mapped_reads_fraction, axis = 0).T\n",
    "    # visualization\n",
    "    sns.clustermap(component_fc,\n",
    "            cbar_kws = {'label': 'FC over total mapped reads'},\n",
    "            metric = 'correlation', cmap = 'Greys', figsize = (4,4))\n",
    "    plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.component_fc.pdf')\n",
    "    \n",
    "    # annotate cluster: elbow method.\n",
    "    anno = annotate_clusters(model_mean, plot = True, model_std = model_std)\n",
    "    plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.elbow_labelling.pdf')\n",
    "    \n",
    "    # calculate entropy and filter, summarize clusters\n",
    "    ent = model_mean.apply(lambda col: entropy(col, mapped_reads_fraction), axis = 0).sort_values()\n",
    "    \n",
    "    # masked\n",
    "    data['contain_mask'] = data.index.isin(mask.index)\n",
    "    fraction_mask = data.groupby(by = 'cluster')['contain_mask'].mean()\n",
    "    \n",
    "    # summarize clusters\n",
    "    f, ax = plt.subplots(1,3, figsize = (12,4))\n",
    "    annotation_summary = pd.concat([anno.sum(axis = 1), ent, fraction_mask], axis = 1)\n",
    "    annotation_summary.columns = ['# RBP assigned', 'RI to total read distribution', 'fraction contain windows needing softmask']\n",
    "    \n",
    "    comp_stats = summaries_annotation_and_mean(anno, model_mean, model_var, mapped_reads_fraction)\n",
    "    annotation_summary = comp_stats.merge(annotation_summary, left_index = True, right_index = True)\n",
    "    annotation_summary['# RBP assigned'].sort_values().plot.barh(ax = ax[0], color = 'grey')\n",
    "    \n",
    "    annotation_summary['RI to total read distribution'].sort_values().plot.barh(ax = ax[1], color = 'grey')\n",
    "    ax[1].set_ylabel('# relative entropy to\\n  total reads distribution')\n",
    "    sns.despine()\n",
    "    \n",
    "    anno.sum(axis = 0).sort_values().plot.barh(ax = ax[2], color = 'grey')\n",
    "    ax[2].set_ylabel('# clusters assigned')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.cluster_summary.pdf')\n",
    "    \n",
    "    # low entropy is bad: should not filter because it still contains some individual binding sites\n",
    "    # too_low_entropy = ent[ent<0.1].index.tolist()\n",
    "    # annotation_summary['filtered']=annotation_summary.index.isin(too_low_entropy)\n",
    "    # anno.loc[anno.index.isin(too_low_entropy)] = False\n",
    "    \n",
    "    annotation_summary.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.cluster_summary.csv')\n",
    "    anno.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.cluster_annotation_binary.csv')\n",
    "    \n",
    "    # calculate effect size: p_bar, fc_bar and p_bar's std\n",
    "    ezik = data[model_mean.columns]\n",
    "    p_bar = np.matmul(ezik, model_mean.T)\n",
    "    p_bar.columns = model_mean.index\n",
    "    p_raw = raw_counts.div(nread_per_window, axis = 0)\n",
    "    fc_bar = p_bar.div(mapped_reads_fraction, axis = 1)\n",
    "    fc_raw = p_raw.div(mapped_reads_fraction, axis = 1)\n",
    "    \n",
    "    var_bar = np.matmul(ezik, model_var.T)\n",
    "    var_bar.columns = model_var.index\n",
    "    std_bar = np.sqrt(var_bar)\n",
    "    \n",
    "    p_bar.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.p_bar.csv')\n",
    "    p_raw.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.p_raw.csv')\n",
    "    fc_bar.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.fc_bar.csv')\n",
    "    fc_raw.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.fc_raw.csv')\n",
    "    std_bar.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.p_bar_std.csv')\n",
    "    \n",
    "    # plot model fit for p_bar and p_raw\n",
    "    f, axes = plt.subplots(2, math.ceil(p_bar.shape[1]/2), figsize = (8, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for col, ax in zip(p_bar.columns, axes):\n",
    "        ax.scatter(p_raw[col], p_bar.loc[p_raw.index, col], color = 'lightgrey', marker = '+')\n",
    "        r, pval = pearsonr(p_raw[col], p_bar.loc[p_raw.index, col])\n",
    "        ax.set_ylabel('p_bar_i')\n",
    "        ax.set_xlabel('p_raw')\n",
    "        ax.set_title(f'{col}\\n r={r:.2f}\\n p={pval:.2E}')\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.p_fit.pdf')\n",
    "    \n",
    "    # visualize regions distribution for each cluster\n",
    "    col = 'feature_type_top'\n",
    "    cluster_count = data.pivot_table(index = 'cluster', columns = col, \n",
    "                                fill_value=0, aggfunc='size')\n",
    "    cluster_frac = cluster_count.div(cluster_count.sum(axis = 1), axis = 0)\n",
    "    \n",
    "    anno_as_color = anno.applymap(lambda ans: 'royalblue' if ans else 'white')\n",
    "    anno_as_color.columns = [c.split('.')[1]+'(labels)' for c in anno_as_color.columns]\n",
    "    sns.clustermap(cluster_frac*100,\n",
    "            cmap = 'Greys', metric = 'cosine', cbar_kws ={'label': '%window', }, cbar_pos = (1,0.2,0.02,0.6),\n",
    "            figsize = (5,6), xticklabels = 1, yticklabels = 1,\n",
    "            row_colors = anno_as_color.T.sort_index().T)\n",
    "    plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.cluster_region_type.pdf')\n",
    "    cluster_count.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.cluster_region_type.csv')\n",
    "    \n",
    "    # Calculate Bayes Factor (hypothesis wise)\n",
    "    comp_mapping = {}\n",
    "    bfs_dmm = []\n",
    "    for name, group in anno.groupby(by = list(anno.columns)):\n",
    "        components = list(group.index)\n",
    "        rbps = anno.columns[list(name)].tolist()\n",
    "        if sum(name) > 0:\n",
    "            print(components, rbps)\n",
    "            bf_df = DMM_bayes_factor(model_alphas, weights, rbps, components, raw_counts)['logLR']\n",
    "            bf_df.name = components[0]\n",
    "            for comp in components:\n",
    "                comp_mapping[comp]=components[0]\n",
    "            bfs_dmm.append(bf_df)\n",
    "    \n",
    "    bfs_dmm = pd.concat(bfs_dmm, axis = 1)\n",
    "    \n",
    "    individual_bfs_dmm = []\n",
    "    \n",
    "    # test individual RBPs' binding\n",
    "    for rbp in anno.columns:\n",
    "        bound_components = anno[rbp][anno[rbp]].index\n",
    "        individual_LR = DMM_bayes_factor(model_alphas, weights, [rbp], bound_components, raw_counts)['logLR']\n",
    "        individual_LR.name = rbp\n",
    "        individual_bfs_dmm.append(individual_LR)\n",
    "    \n",
    "    individual_bfs_dmm = pd.concat(individual_bfs_dmm, axis = 1)\n",
    "    bfs_dmm.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.BF_hypothesis.csv')\n",
    "    individual_bfs_dmm.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.BF_individual.csv') \n",
    "    \n",
    "    # ====== By Hypothesis =====\n",
    "    data_bf_dmm, dmm_fcount = filter_by_bf(bfs_dmm, individual_bfs_dmm, data = data, anno = anno, comp_mapping = comp_mapping)\n",
    "    dmm_fcount.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.rbp_region_type.BF_filtered.csv')\n",
    "    \n",
    "    # calculate jaccard index\n",
    "    if not data_bf_dmm[anno.columns].sum(axis = 0).ge(1).all():\n",
    "        # if not everything has at least 1 binding site\n",
    "        cols_to_plot = anno.columns[data_bf_dmm[anno.columns].sum(axis = 0).ge(1)]\n",
    "    else:\n",
    "        cols_to_plot = anno.columns\n",
    "    \n",
    "    if len(cols_to_plot)>1:\n",
    "        d_dmm = compute_jaccard_index(data_bf_dmm[anno.columns])\n",
    "        cm=sns.clustermap(d_dmm, cmap = 'Greys', metric = 'correlation', figsize = (4,4),\n",
    "                    vmax = 1)\n",
    "        cm.cax.set_visible(False)\n",
    "        plt.suptitle('ABC(Dirichlet Mixture Model: BF filtered)', y = 1)\n",
    "        plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.jaccard_index.pdf')\n",
    "    else:\n",
    "        print('RBP with binding site:', cols_to_plot)\n",
    "    \n",
    "    # count RBP\n",
    "    try:\n",
    "        sns.set_palette('tab20c')\n",
    "        plt.style.use('seaborn-white')\n",
    "        dmm_fcount.plot.barh(stacked = True)\n",
    "        dmm_fcount.to_csv(basedir / 'DMM'/ f'{dmm_prefix}.rbp_region_type.csv')\n",
    "        sns.despine()\n",
    "        plt.xlabel('# windows')\n",
    "        plt.legend(bbox_to_anchor=(1.2,1))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.region_type_count_by_hypothesis_RBPwise.pdf')\n",
    "    except Exception as e:\n",
    "        print('Fail to plot *_rbp_region_type', e)\n",
    "    \n",
    "    # count by hypothesis\n",
    "    try:\n",
    "        fcount_by_hypothesis = data_bf_dmm.groupby(by = ['BF_assignment'])['feature_type_top'].value_counts().unstack().fillna(0)\n",
    "        fcount_by_hypothesis.index = fcount_by_hypothesis.index.map(lambda component: component+':'+','.join(\n",
    "            [r.split('.')[1] for r in anno.columns[anno.loc[component]].tolist()]))\n",
    "        fcount_by_hypothesis.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.hypothesis_region_type.csv')\n",
    "        fcount_by_hypothesis.plot.barh(stacked = True)\n",
    "        sns.despine()\n",
    "        plt.legend(bbox_to_anchor=(1.2,1))\n",
    "        plt.xlabel('# windows')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.region_type_count_by_hypothesis.pdf')\n",
    "    except Exception as e:\n",
    "        print('Fail to plot *_hypothesis_region_type', e)\n",
    "    \n",
    "    # ====== By Individual RBPs ======\n",
    "    data_bf_dmm.loc[individual_bfs_dmm.index, anno.columns]=individual_bfs_dmm.ge(logLR_threshold)\n",
    "    \n",
    "    # perform masking\n",
    "    data_bf_dmm['name'] = data_bf_dmm.index\n",
    "    data_bf_dmm_masked = mask_megaoutput(data_bf_dmm, mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # jaccard index\n",
    "    try:\n",
    "        d_dmm = compute_jaccard_index(data_bf_dmm_masked.loc[:, data_bf_dmm_masked.columns.str.startswith(out_stem)])\n",
    "        cm=sns.clustermap(d_dmm, cmap = 'Greys', metric = 'correlation', figsize = (4,4),\n",
    "                    vmax = 0.3)\n",
    "        cm.cax.set_visible(False)\n",
    "        plt.suptitle('ABC(Dirichlet Mixture Model: individual BF)', y = 1)\n",
    "    \n",
    "        indiv_assignment_fcount_unmasked = count_by_rbp(data_bf_dmm, anno)\n",
    "        indiv_assignment_fcount = count_by_rbp(data_bf_dmm_masked, anno)\n",
    "        indiv_assignment_fcount.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.region_type_count_by_individual.csv')\n",
    "        indiv_assignment_fcount.T.sort_index().T.plot.barh(stacked = True)\n",
    "        sns.despine()\n",
    "        plt.legend(bbox_to_anchor=(1,1))\n",
    "        plt.xlabel('# genomic window')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.region_type_count_by_individual.pdf')\n",
    "        \n",
    "        # plot masked: region type\n",
    "        f, axes = plt.subplots(1,2, sharex = True, sharey = True, figsize = (6,3))\n",
    "        diff = (indiv_assignment_fcount_unmasked-indiv_assignment_fcount).fillna(0)\n",
    "        diff = (indiv_assignment_fcount_unmasked-indiv_assignment_fcount).fillna(0)\n",
    "        diff.loc[diff.sum(axis =1).sort_values().index, diff.sum(axis = 0)>0].plot.barh(\n",
    "            stacked = True, ax = axes[0])\n",
    "        diff.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.region_type_masked.csv')\n",
    "        \n",
    "        \n",
    "        # plot masked: transcript type\n",
    "        indiv_assignment_fcount = count_by_rbp(data_bf_dmm_masked, anno, to_count = 'transcript_type_top')\n",
    "        indiv_assignment_fcount_unmasked = count_by_rbp(data_bf_dmm, anno, to_count = 'transcript_type_top')\n",
    "        diff = (indiv_assignment_fcount_unmasked-indiv_assignment_fcount).fillna(0)\n",
    "        diff.loc[diff.sum(axis =1).sort_values().index, diff.sum(axis = 0)>0].plot.barh(\n",
    "            stacked = True, ax = axes[1])\n",
    "        diff.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.transcript_type_masked.csv')\n",
    "        sns.despine()\n",
    "        ax[0].legend(bbox_to_anchor=(1,1))\n",
    "        ax[1].legend(bbox_to_anchor=(1,1))\n",
    "        plt.xlabel('# genomic window masked')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.region_type_masked.pdf')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Fail to plot jaccard and region_type', e)\n",
    "    \n",
    "        # ====== Generate Output ======\n",
    "        # --- output by individual RBP ---\n",
    "        columns = ['chrom', 'start', 'end', 'name', 'score',\n",
    "        'strand', 'feature_id', 'feature_bin', 'feature_type_top',\n",
    "        'feature_types', 'gene_name', 'gene_id', 'transcript_ids',\n",
    "        'gene_type_top', 'transcript_type_top', 'gene_types',\n",
    "        'transcript_types','logLR', 'BF_assignment',]\n",
    "        # output per RBP enrich windows\n",
    "    \n",
    "    for c in anno.columns:\n",
    "    \n",
    "        enriched_windows = data_bf_dmm_masked.loc[data_bf_dmm_masked[c], columns]\n",
    "        enriched_windows['p_bar']=p_bar[c]\n",
    "        enriched_windows['p_raw']=p_raw[c]\n",
    "        enriched_windows['fc_raw']=fc_raw[c]\n",
    "        enriched_windows['fc_bar']=fc_bar[c]\n",
    "        enriched_windows['p_bar_std']=std_bar[c]\n",
    "        enriched_windows.to_csv(dmm_param_dir / 'DMM'/ f'{c}.enriched_windows.tsv', sep = '\\t')\n",
    "        print(f'found {c} enriched windows:', enriched_windows.shape)\n",
    "    \n",
    "    # --- output everything ---\n",
    "    # output the full data\n",
    "    p_bar.columns=[f'p_bar:{c}' for c in p_bar.columns]\n",
    "    p_raw.columns=[f'p_raw:{c}' for c in p_raw.columns]\n",
    "    fc_raw.columns=[f'fc_raw:{c}' for c in fc_raw.columns]\n",
    "    fc_bar.columns=[f'fc_bar:{c}' for c in fc_bar.columns]\n",
    "    std_bar.columns=[f'p_bar_std:{c}' for c in std_bar.columns]\n",
    "    individual_bfs_dmm.columns=[f'logLR:{c}' for c in individual_bfs_dmm.columns]\n",
    "    \n",
    "    # concat everything and write together\n",
    "    megaoutput = pd.concat([data_bf_dmm_masked, p_bar, p_raw, fc_raw, fc_bar, std_bar, individual_bfs_dmm], axis = 1)\n",
    "    megaoutput.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.megaoutputs.tsv', sep = '\\t')\n",
    "    \n",
    "    data_bf_dmm.to_csv(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.megaoutputs_unmasked.tsv', sep = '\\t')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f407f-785f-493b-a2fc-e09cac6746e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Fraction mapped reads: \n",
      " ======== K562_rep6.PUM2       0.012202\n",
      "K562_rep6.EIF3G      0.021944\n",
      "K562_rep6.ZC3H11A    0.046280\n",
      "K562_rep6.DDX3       0.052620\n",
      "K562_rep6.LIN28B     0.061494\n",
      "K562_rep6.SF3B4      0.082945\n",
      "K562_rep6.IGF2BP2    0.101528\n",
      "K562_rep6.PRPF8      0.123238\n",
      "K562_rep6.RBFOX2     0.125049\n",
      "K562_rep6.FAM120A    0.372699\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tscc/nfs/home/hsher/miniconda3/envs/tensorflow_prob/lib/python3.10/site-packages/seaborn/utils.py:80: UserWarning: Glyph 8 ) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_212845/2055497912.py:54: UserWarning: Glyph 8 ) missing from current font.\n",
      "  plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.model_mean.pdf')\n",
      "/tmp/ipykernel_212845/2055497912.py:54: UserWarning: Glyph 8 ) missing from current font.\n",
      "  plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.model_mean.pdf')\n",
      "/tmp/ipykernel_212845/2055497912.py:104: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
      "  p_bar = np.matmul(ezik, model_mean.T)\n",
      "/tmp/ipykernel_212845/2055497912.py:110: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
      "  var_bar = np.matmul(ezik, model_var.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V6'] ['K562_rep6.SF3B4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 10:19:56.414186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V3'] ['K562_rep6.ZC3H11A', 'K562_rep6.PRPF8', 'K562_rep6.LIN28B']\n",
      "['V2'] ['K562_rep6.FAM120A']\n",
      "['V1'] ['K562_rep6.PUM2', 'K562_rep6.DDX3', 'K562_rep6.ZC3H11A', 'K562_rep6.EIF3G', 'K562_rep6.LIN28B']\n",
      "['V5'] ['K562_rep6.RBFOX2', 'K562_rep6.ZC3H11A']\n",
      "['V4'] ['K562_rep6.IGF2BP2', 'K562_rep6.PUM2', 'K562_rep6.FAM120A', 'K562_rep6.ZC3H11A', 'K562_rep6.LIN28B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_212845/2055497912.py:202: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-white')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to plot jaccard and region_type 'Axes' object is not subscriptable\n",
      "found K562_rep6.IGF2BP2 enriched windows: (3024, 24)\n",
      "found K562_rep6.RBFOX2 enriched windows: (7310, 24)\n",
      "found K562_rep6.PUM2 enriched windows: (41, 24)\n",
      "found K562_rep6.FAM120A enriched windows: (46338, 24)\n",
      "found K562_rep6.DDX3 enriched windows: (4202, 24)\n",
      "found K562_rep6.ZC3H11A enriched windows: (6275, 24)\n",
      "found K562_rep6.EIF3G enriched windows: (528, 24)\n",
      "found K562_rep6.PRPF8 enriched windows: (19833, 24)\n",
      "found K562_rep6.LIN28B enriched windows: (12910, 24)\n",
      "found K562_rep6.SF3B4 enriched windows: (5095, 24)\n",
      "========Fraction mapped reads: \n",
      " ======== K562_rep6.PUM2       0.012202\n",
      "K562_rep6.EIF3G      0.021944\n",
      "K562_rep6.ZC3H11A    0.046280\n",
      "K562_rep6.DDX3       0.052620\n",
      "K562_rep6.LIN28B     0.061494\n",
      "K562_rep6.SF3B4      0.082945\n",
      "K562_rep6.IGF2BP2    0.101528\n",
      "K562_rep6.PRPF8      0.123238\n",
      "K562_rep6.RBFOX2     0.125049\n",
      "K562_rep6.FAM120A    0.372699\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tscc/nfs/home/hsher/miniconda3/envs/tensorflow_prob/lib/python3.10/site-packages/seaborn/utils.py:80: UserWarning: Glyph 8 ) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_212845/2055497912.py:54: UserWarning: Glyph 8 ) missing from current font.\n",
      "  plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.model_mean.pdf')\n",
      "/tmp/ipykernel_212845/2055497912.py:54: UserWarning: Glyph 8 ) missing from current font.\n",
      "  plt.savefig(dmm_param_dir / 'DMM'/ f'{dmm_prefix}.model_mean.pdf')\n",
      "/tmp/ipykernel_212845/2055497912.py:104: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
      "  p_bar = np.matmul(ezik, model_mean.T)\n",
      "/tmp/ipykernel_212845/2055497912.py:110: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
      "  var_bar = np.matmul(ezik, model_var.T)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,10):\n",
    "    analyze_dmm_prefix(dmm_param_dir, f'K562_rep6.{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03483cb8-2e19-4d48-bc2b-dc7591f3cc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_prob",
   "language": "python",
   "name": "tensorflow_prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
